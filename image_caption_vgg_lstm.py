# -*- coding: utf-8 -*-
"""Image_caption_vgg_lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GLeq9SmpdFG76i7ClTs6hTtTrvKcSw6o
"""

#mount the drive first from google.colab import drive
from google.colab import drive
drive.mount('/content/drive/', force_remount= True)

! pwd

import os
os.chdir('/content/drive/My Drive/Colab Notebooks/Caption_Gen_VGG_LSTM/')

! pwd

BASE_DIR = '/content/drive/My Drive/Colab Notebooks/Caption_Gen_VGG_LSTM/'
WORKING_DIR = BASE_DIR

# won't need to scroll left-right to see the output.

from IPython.display import HTML, display

def set_css():
  display(HTML('''
  <style>
    pre {
        white-space: pre-wrap;
    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell', set_css)

import tensorflow as tf
device_list = tf.test.gpu_device_name()
device_list

!nvidia-smi

import tensorflow as tf
device_list = tf.test.gpu_device_name()
device_list



import os
import pickle
import numpy as np
import pandas as pd
import tensorflow as tf
from tqdm.notebook import tqdm


from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add

import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from textwrap import wrap

# vgg16 model
model = VGG16()
# restructure
model = Model(inputs=model.inputs, outputs=model.layers[-2].output)
print(model.summary())

# extract features from image

features = {}
directory = os.path.join(BASE_DIR, 'images')

for img_name in tqdm(os.listdir(directory)):
    img_path = directory + '/' + img_name
    image = load_img(img_path, target_size=(224, 224))
    image = img_to_array(image)
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
    image = preprocess_input(image)
    feature = model.predict(image, verbose=0)
    image_id = img_name
    features[image_id] = feature

# store features in pickle
pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))

# load features from pickle
with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:
    features = pickle.load(f)

# Access any images feature
features['3336808362_c17837afd8.jpg']

import pickle

#  path to the features.pkl file
file_path = WORKING_DIR+'features.pkl'

# Load
with open(file_path, 'rb') as f:
    features = pickle.load(f)

# Print first five entries
for i, (key, value) in enumerate(features.items()):
    print(f"Entry {i+1}:")
    print(f"Key: {key}")
    print(f"Value: {value}")
    if i == 4:  # Stop after printing the first five entries
        break

"""**Load Captions**"""

with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:
    next(f) #skips the first line
    captions_doc = f.read()

# create mapping of image to captions

mapping = {}
for line in tqdm(captions_doc.split('\n')):
    tokens = line.split(',')
    if len(line) < 2:
        continue
    image_id, caption = tokens[0], tokens[1:]
    image_id = image_id
    caption = " ".join(caption)
    if image_id not in mapping:
        mapping[image_id] = []
    mapping[image_id].append(caption)

len(mapping)

# Assuming `mapping` is your dictionary
first_five_entries = dict(list(mapping.items())[:5])
print(first_five_entries)

# 69189650_6687da7280.jpg (Pink Girl Image)
from PIL import Image
import matplotlib.pyplot as plt

def show_image(image_path):
    # Load the image
    img = Image.open(image_path)
    # Display the image
    plt.imshow(img)
    plt.axis('off')
    plt.show()

# Example image path relative to the base directory
image_path = os.path.join(BASE_DIR, "images/1000268201_693b08cb0e.jpg")

# Show the image
show_image(image_path)

mapping['1000268201_693b08cb0e.jpg']

# preprocess the text

def clean(mapping):
    # Iterate through each key-value pair (image ID and captions) in the mapping dictionary
    for key, captions in mapping.items():
        # Iterate through each caption in the list of captions for the current image (We've 5 for each)
        for i in range(len(captions)):
            caption = captions[i]  # Get the current caption
            caption = caption.lower()  # Convert the caption to lowercase

            # Remove non-alphabetic characters from the caption using a regular expression
            caption = caption.replace('[^A-Za-z]', '')

            # Remove extra whitespace characters using a regular expression
            caption = caption.replace('\s+', ' ')

            # Add start and end sequence tokens to the caption to indicate the beginning and end of a sentence
            caption = 'startseq ' + " ".join([word for word in caption.split() if len(word)>1]) + ' endseq'

            # Update the caption in the list of captions
            captions[i] = caption  # Assign the cleaned caption back to its position in the list

clean(mapping)

mapping['1000268201_693b08cb0e.jpg']

# Dumping Mapping

import json

# Dump the mapping dictionary to a JSON file
with open('mapping.json', 'w') as f:
    json.dump(mapping, f)

import json

# Load the mapping dictionary from the JSON file
with open('mapping.json', 'r') as f:
    mapping = json.load(f)

all_captions = []
for key in mapping:
    for caption in mapping[key]:
        all_captions.append(caption)

len(all_captions)

all_captions[:10]

# tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_captions)
vocab_size = len(tokenizer.word_index) + 1

vocab_size

# maximum length of the caption available
max_length = max(len(caption.split()) for caption in all_captions)

max_length





image_ids = list(mapping.keys())
split = int(len(image_ids) * 0.90)
train = image_ids[:split]
test = image_ids[split:]

# creating data generator to get data in batch

def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):
    X1, X2, y = list(), list(), list()
    n = 0
    while 1:
        for key in data_keys:
            if key not in features:
                continue  # Skip this key if it's not present in features
            n += 1
            captions = mapping[key]
            for caption in captions:
                seq = tokenizer.texts_to_sequences([caption])[0]
                for i in range(1, len(seq)):
                    in_seq, out_seq = seq[:i], seq[i]
                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]
                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]

                    X1.append(features[key][0])
                    X2.append(in_seq)
                    y.append(out_seq)
            if n == batch_size:
                X1, X2, y = np.array(X1), np.array(X2), np.array(y)
                yield [X1, X2], y
                X1, X2, y = list(), list(), list()
                n = 0



# encoder model
# image feature layers
inputs1 = Input(shape=(4096,))
fe1 = Dropout(0.4)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
# sequence feature layers
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
se2 = Dropout(0.4)(se1)
se3 = LSTM(256)(se2)


# decoder model
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)

model = Model(inputs=[inputs1, inputs2], outputs=outputs)
model.compile(loss='categorical_crossentropy', optimizer='adam')

plot_model(model, show_shapes=True)



epochs = 50
batch_size = 64
steps = len(train) // batch_size

for i in range(epochs):
    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)
    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)

epochs = 50
batch_size = 64
steps = len(train) // batch_size

for i in range(epochs):
    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)
    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)

model.save(WORKING_DIR+'/best_model.h5')

from keras.models import load_model


# Load the saved model
model = load_model(WORKING_DIR+'/best_model.h5')

"""**GenerateCAptions for Image**"""

def idx_to_word(integer, tokenizer):
    for word, index in tokenizer.word_index.items():
        if index == integer:
            return word
    return None

# generate caption for an image
def predict_caption(model, image, tokenizer, max_length):
    in_text = 'startseq'
    for i in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], max_length)
        yhat = model.predict([image, sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = idx_to_word(yhat, tokenizer)
        if word is None:
            break
        in_text += " " + word
        if word == 'endseq':
            break

    return in_text

from PIL import Image
import matplotlib.pyplot as plt
def generate_caption(image_name):

    image_id = image_name
    img_path = os.path.join(BASE_DIR, "images", image_name)
    image = Image.open(img_path)
    captions = mapping[image_id]
    # print('---------------------Actual---------------------')
    # for caption in captions:
    #     print(caption)

    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)
    print('--------------------Predicted--------------------')
    print(y_pred)
    plt.imshow(image)
    plt.axis('off')

generate_caption("347543966_b2053ae78c.jpg")

generate_caption("1002674143_1b742ab4b8.jpg")



"""**Real Image**"""

vgg_model = VGG16()
# restructure the model
vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)

image_name = "Laptop.jpg"
image_path = os.path.join(BASE_DIR, "Images", image_name)
# load image
image = load_img(image_path, target_size=(224, 224))
# convert image pixels to numpy array
image = img_to_array(image)
# reshape data for model
image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
# preprocess image for vgg
image = preprocess_input(image)
# extract features
feature = vgg_model.predict(image, verbose=0)
# predict from the trained model
predict_caption(model, feature, tokenizer, max_length)

import matplotlib.pyplot as plt
img = Image.open(image_path)
plt.imshow(img)
plt.axis('off')



from PIL import Image
import matplotlib.pyplot as plt

def generate_caption_and_display(image_path, model, tokenizer, max_length, vgg_model):
    # Load image
    image = load_img(image_path, target_size=(224, 224))

    # Convert image pixels to numpy array
    image = img_to_array(image)

    # Reshape data for model
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))

    # Preprocess image for VGG
    image = preprocess_input(image)

    # Extract features
    feature = vgg_model.predict(image, verbose=0)

    # Predict caption
    caption = predict_caption(model, feature, tokenizer, max_length)

    # Display image
    img = Image.open(image_path)
    plt.imshow(img)
    plt.axis('off')

    # Print predicted caption
    print('Predicted Caption:', caption)

# Example usage
image_name = "Laptop.jpg"
image_path = os.path.join(BASE_DIR, "Images", image_name)
generate_caption_and_display(image_path, model, tokenizer, max_length, vgg_model)

image_name2 = "tree.jpg"
image_path2 = os.path.join(BASE_DIR, "Images", image_name2)
generate_caption_and_display(image_path2, model, tokenizer, max_length, vgg_model)

image_name2 = "study.jpg"
image_path2 = os.path.join(BASE_DIR, "Images", image_name2)
generate_caption_and_display(image_path2, model, tokenizer, max_length, vgg_model)

image_name2 = "notebook.jpg"
image_path2 = os.path.join(BASE_DIR, "Images", image_name2)
generate_caption_and_display(image_path2, model, tokenizer, max_length, vgg_model)

image_name2 = "notebook.jpg"
image_path2 = os.path.join(BASE_DIR, "Images", image_name2)
generate_caption_and_display(image_path2, model, tokenizer, max_length, vgg_model)

Cap= "startseq the girl is doing the picture of the picture endseq"

Cap = Cap.replace("startseq ", "").replace(" endseq", "")

Cap